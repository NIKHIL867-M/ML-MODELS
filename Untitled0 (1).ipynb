{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n-MuZ8TI7Z3O",
        "outputId": "df609b32-ef4b-4315-b69e-329ebff95ddb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Correct file path (adjust this if needed based on Google Drive window)\n",
        "zip_path = '/content/drive/MyDrive/dataset (1).zip'\n",
        "extract_path = '/content/dataset'\n",
        "\n",
        "# Unzip the file\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "print(\"Unzipped to\", extract_path)\n",
        "\n",
        "# Optional: List a few files to confirm extraction\n",
        "for root, dirs, files in os.walk(extract_path):\n",
        "    for name in files:\n",
        "        print(os.path.join(root, name))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fih81ljL7wOG",
        "outputId": "8341f65a-281a-463a-9062-065b54e55878"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unzipped to /content/dataset\n",
            "/content/dataset/dataset/test/digital_behavior.csv\n",
            "/content/dataset/dataset/test/asset_returns.csv\n",
            "/content/dataset/dataset/test/current_products.csv\n",
            "/content/dataset/dataset/test/wearables.csv\n",
            "/content/dataset/dataset/test/users.csv\n",
            "/content/dataset/dataset/test/claims_history_test_public.csv\n",
            "/content/dataset/dataset/test/cost_structure.csv\n",
            "/content/dataset/dataset/test/telematics.csv\n",
            "/content/dataset/dataset/train/digital_behavior.csv\n",
            "/content/dataset/dataset/train/asset_returns.csv\n",
            "/content/dataset/dataset/train/current_products.csv\n",
            "/content/dataset/dataset/train/wearables.csv\n",
            "/content/dataset/dataset/train/users.csv\n",
            "/content/dataset/dataset/train/cost_structure.csv\n",
            "/content/dataset/dataset/train/claims_history.csv\n",
            "/content/dataset/dataset/train/telematics.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "base_dir = '/content/dataset/dataset'\n",
        "train_dir = os.path.join(base_dir, 'train')\n",
        "test_dir = os.path.join(base_dir, 'test')\n",
        "\n",
        "def load_all_csvs(folder):\n",
        "    return {os.path.splitext(f)[0]: pd.read_csv(os.path.join(folder, f))\n",
        "            for f in os.listdir(folder) if f.endswith('.csv')}\n",
        "\n",
        "train_dfs = load_all_csvs(train_dir)\n",
        "test_dfs = load_all_csvs(test_dir)\n",
        "print('Train tables:', list(train_dfs.keys()))\n",
        "print('Test tables:', list(test_dfs.keys()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CXdmS3Km7wXc",
        "outputId": "089c98ee-aac5-4602-e97d-40a871dbac2f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train tables: ['digital_behavior', 'asset_returns', 'current_products', 'wearables', 'users', 'cost_structure', 'claims_history', 'telematics']\n",
            "Test tables: ['digital_behavior', 'asset_returns', 'current_products', 'wearables', 'users', 'claims_history_test_public', 'cost_structure', 'telematics']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for name, df in train_dfs.items():\n",
        "    print(f\"\\n{name} shape:\", df.shape)\n",
        "    print(df.info())\n",
        "    print(\"Missing data:\\n\", df.isnull().sum().sort_values(ascending=False).head())\n",
        "    print(df.describe(include='all').T.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l1FlFvk3-BhB",
        "outputId": "6c90d55c-1f9c-4b1e-dc04-8730975b8b0b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "digital_behavior shape: (1050000, 7)\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1050000 entries, 0 to 1049999\n",
            "Data columns (total 7 columns):\n",
            " #   Column                  Non-Null Count    Dtype  \n",
            "---  ------                  --------------    -----  \n",
            " 0   user_id                 1050000 non-null  int64  \n",
            " 1   week_timestamp          1050000 non-null  object \n",
            " 2   daily_screen_time_avg   1050000 non-null  float64\n",
            " 3   social_media_time_avg   1050000 non-null  float64\n",
            " 4   app_usage_vector        1050000 non-null  object \n",
            " 5   social_sentiment_score  1050000 non-null  float64\n",
            " 6   ecommerce_spend_vector  1050000 non-null  object \n",
            "dtypes: float64(3), int64(1), object(3)\n",
            "memory usage: 56.1+ MB\n",
            "None\n",
            "Missing data:\n",
            " user_id                  0\n",
            "week_timestamp           0\n",
            "daily_screen_time_avg    0\n",
            "social_media_time_avg    0\n",
            "app_usage_vector         0\n",
            "dtype: int64\n",
            "                           count   unique  \\\n",
            "user_id                1050000.0      NaN   \n",
            "week_timestamp           1050000      105   \n",
            "daily_screen_time_avg  1050000.0      NaN   \n",
            "social_media_time_avg  1050000.0      NaN   \n",
            "app_usage_vector         1050000  1026022   \n",
            "\n",
            "                                                                     top  \\\n",
            "user_id                                                              NaN   \n",
            "week_timestamp                                                2023-01-01   \n",
            "daily_screen_time_avg                                                NaN   \n",
            "social_media_time_avg                                                NaN   \n",
            "app_usage_vector       {\"Finance\": 0.1, \"Shopping\": 0.31, \"Entertainm...   \n",
            "\n",
            "                        freq      mean          std  min      25%     50%  \\\n",
            "user_id                  NaN    4999.5  2886.752706  0.0  2499.75  4999.5   \n",
            "week_timestamp         10000       NaN          NaN  NaN      NaN     NaN   \n",
            "daily_screen_time_avg    NaN  6.685618     1.588161  2.0     5.62    6.68   \n",
            "social_media_time_avg    NaN  2.855658     0.769727  1.0     2.34    2.87   \n",
            "app_usage_vector          11       NaN          NaN  NaN      NaN     NaN   \n",
            "\n",
            "                           75%     max  \n",
            "user_id                7499.25  9999.0  \n",
            "week_timestamp             NaN     NaN  \n",
            "daily_screen_time_avg     7.76   14.03  \n",
            "social_media_time_avg     3.37    6.14  \n",
            "app_usage_vector           NaN     NaN  \n",
            "\n",
            "asset_returns shape: (6, 3)\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 6 entries, 0 to 5\n",
            "Data columns (total 3 columns):\n",
            " #   Column              Non-Null Count  Dtype  \n",
            "---  ------              --------------  -----  \n",
            " 0   asset_class         6 non-null      object \n",
            " 1   mean_annual_return  6 non-null      float64\n",
            " 2   annual_volatility   6 non-null      float64\n",
            "dtypes: float64(2), object(1)\n",
            "memory usage: 276.0+ bytes\n",
            "None\n",
            "Missing data:\n",
            " asset_class           0\n",
            "mean_annual_return    0\n",
            "annual_volatility     0\n",
            "dtype: int64\n",
            "                   count unique        top freq      mean       std   min  \\\n",
            "asset_class            6      6  US Stocks    1       NaN       NaN   NaN   \n",
            "mean_annual_return   6.0    NaN        NaN  NaN  0.099167  0.041763  0.03   \n",
            "annual_volatility    6.0    NaN        NaN  NaN  0.258333  0.164853  0.05   \n",
            "\n",
            "                        25%    50%     75%   max  \n",
            "asset_class             NaN    NaN     NaN   NaN  \n",
            "mean_annual_return  0.08625    0.1   0.125  0.15  \n",
            "annual_volatility    0.1725  0.215  0.3625   0.5  \n",
            "\n",
            "current_products shape: (10000, 8)\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 10000 entries, 0 to 9999\n",
            "Data columns (total 8 columns):\n",
            " #   Column                      Non-Null Count  Dtype  \n",
            "---  ------                      --------------  -----  \n",
            " 0   user_id                     10000 non-null  int64  \n",
            " 1   has_auto_insurance          10000 non-null  bool   \n",
            " 2   premium_auto                10000 non-null  float64\n",
            " 3   has_renters_insurance       10000 non-null  bool   \n",
            " 4   has_pet_insurance           10000 non-null  bool   \n",
            " 5   has_life_insurance          10000 non-null  bool   \n",
            " 6   investment_portfolio_value  10000 non-null  float64\n",
            " 7   portfolio_asset_allocation  10000 non-null  object \n",
            "dtypes: bool(4), float64(2), int64(1), object(1)\n",
            "memory usage: 351.7+ KB\n",
            "None\n",
            "Missing data:\n",
            " user_id                  0\n",
            "has_auto_insurance       0\n",
            "premium_auto             0\n",
            "has_renters_insurance    0\n",
            "has_pet_insurance        0\n",
            "dtype: int64\n",
            "                         count unique    top  freq        mean         std  \\\n",
            "user_id                10000.0    NaN    NaN   NaN      4999.5  2886.89568   \n",
            "has_auto_insurance       10000      2  False  5992         NaN         NaN   \n",
            "premium_auto           10000.0    NaN    NaN   NaN  277.565093   364.36638   \n",
            "has_renters_insurance    10000      2  False  6984         NaN         NaN   \n",
            "has_pet_insurance        10000      2  False  5995         NaN         NaN   \n",
            "\n",
            "                       min      25%     50%      75%      max  \n",
            "user_id                0.0  2499.75  4999.5  7499.25   9999.0  \n",
            "has_auto_insurance     NaN      NaN     NaN      NaN      NaN  \n",
            "premium_auto           0.0      0.0     0.0  602.855  1742.09  \n",
            "has_renters_insurance  NaN      NaN     NaN      NaN      NaN  \n",
            "has_pet_insurance      NaN      NaN     NaN      NaN      NaN  \n",
            "\n",
            "wearables shape: (3655000, 6)\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 3655000 entries, 0 to 3654999\n",
            "Data columns (total 6 columns):\n",
            " #   Column                Dtype  \n",
            "---  ------                -----  \n",
            " 0   user_id               int64  \n",
            " 1   date                  object \n",
            " 2   daily_steps           int64  \n",
            " 3   resting_heart_rate    int64  \n",
            " 4   sleep_duration_hours  float64\n",
            " 5   active_minutes        int64  \n",
            "dtypes: float64(1), int64(4), object(1)\n",
            "memory usage: 167.3+ MB\n",
            "None\n",
            "Missing data:\n",
            " user_id                 0\n",
            "date                    0\n",
            "daily_steps             0\n",
            "resting_heart_rate      0\n",
            "sleep_duration_hours    0\n",
            "dtype: int64\n",
            "                          count unique         top  freq         mean  \\\n",
            "user_id               3655000.0    NaN         NaN   NaN    4979.2318   \n",
            "date                    3655000    731  2024-12-31  5000          NaN   \n",
            "daily_steps           3655000.0    NaN         NaN   NaN  5105.800733   \n",
            "resting_heart_rate    3655000.0    NaN         NaN   NaN    66.285973   \n",
            "sleep_duration_hours  3655000.0    NaN         NaN   NaN     6.954648   \n",
            "\n",
            "                              std   min     25%     50%      75%      max  \n",
            "user_id               2882.770341   0.0  2475.5  4957.5  7461.25   9996.0  \n",
            "date                          NaN   NaN     NaN     NaN      NaN      NaN  \n",
            "daily_steps           2268.702986   0.0  3543.0  5095.0   6647.0  16165.0  \n",
            "resting_heart_rate       5.586625  42.0    62.0    66.0     70.0     89.0  \n",
            "sleep_duration_hours     0.978763   3.0    6.29    6.95     7.62    11.92  \n",
            "\n",
            "users shape: (10000, 19)\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 10000 entries, 0 to 9999\n",
            "Data columns (total 19 columns):\n",
            " #   Column                         Non-Null Count  Dtype  \n",
            "---  ------                         --------------  -----  \n",
            " 0   user_id                        10000 non-null  int64  \n",
            " 1   age                            10000 non-null  int64  \n",
            " 2   gender_identity                10000 non-null  object \n",
            " 3   ethnicity                      10000 non-null  object \n",
            " 4   zip_code                       10000 non-null  int64  \n",
            " 5   education_level                10000 non-null  object \n",
            " 6   is_student                     10000 non-null  bool   \n",
            " 7   employment_status              10000 non-null  object \n",
            " 8   has_children                   10000 non-null  bool   \n",
            " 9   is_married                     10000 non-null  bool   \n",
            " 10  annual_income                  10000 non-null  float64\n",
            " 11  side_hustle_income             10000 non-null  float64\n",
            " 12  student_loan_debt              10000 non-null  float64\n",
            " 13  credit_card_debt               10000 non-null  float64\n",
            " 14  bnpl_usage_frequency           10000 non-null  int64  \n",
            " 15  financial_stress_coping_style  10000 non-null  object \n",
            " 16  savings_rate                   10000 non-null  float64\n",
            " 17  risk_tolerance_score           10000 non-null  int64  \n",
            " 18  financial_literacy_score       10000 non-null  int64  \n",
            "dtypes: bool(3), float64(5), int64(6), object(5)\n",
            "memory usage: 1.2+ MB\n",
            "None\n",
            "Missing data:\n",
            " user_id            0\n",
            "age                0\n",
            "gender_identity    0\n",
            "ethnicity          0\n",
            "zip_code           0\n",
            "dtype: int64\n",
            "                   count unique     top  freq        mean           std  \\\n",
            "user_id          10000.0    NaN     NaN   NaN      4999.5    2886.89568   \n",
            "age              10000.0    NaN     NaN   NaN     23.0245      2.917079   \n",
            "gender_identity    10000      3  Female  4915         NaN           NaN   \n",
            "ethnicity          10000      5   White  5044         NaN           NaN   \n",
            "zip_code         10000.0    NaN     NaN   NaN  55183.7353  26051.085908   \n",
            "\n",
            "                     min      25%      50%       75%      max  \n",
            "user_id              0.0  2499.75   4999.5   7499.25   9999.0  \n",
            "age                 18.0     21.0     23.0      26.0     28.0  \n",
            "gender_identity      NaN      NaN      NaN       NaN      NaN  \n",
            "ethnicity            NaN      NaN      NaN       NaN      NaN  \n",
            "zip_code         10030.0  32789.5  55144.5  77883.75  99942.0  \n",
            "\n",
            "cost_structure shape: (7, 2)\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 7 entries, 0 to 6\n",
            "Data columns (total 2 columns):\n",
            " #   Column     Non-Null Count  Dtype  \n",
            "---  ------     --------------  -----  \n",
            " 0   cost_type  7 non-null      object \n",
            " 1   value      7 non-null      float64\n",
            "dtypes: float64(1), object(1)\n",
            "memory usage: 244.0+ bytes\n",
            "None\n",
            "Missing data:\n",
            " cost_type    0\n",
            "value        0\n",
            "dtype: int64\n",
            "          count unique                      top freq          mean  \\\n",
            "cost_type     7      7  claims_processing_fixed    1           NaN   \n",
            "value       7.0    NaN                      NaN  NaN  71560.717143   \n",
            "\n",
            "                     std   min   25%    50%    75%       max  \n",
            "cost_type            NaN   NaN   NaN    NaN    NaN       NaN  \n",
            "value      188924.014469  0.02  62.5  150.0  325.0  500000.0  \n",
            "\n",
            "claims_history shape: (3137, 6)\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 3137 entries, 0 to 3136\n",
            "Data columns (total 6 columns):\n",
            " #   Column          Non-Null Count  Dtype  \n",
            "---  ------          --------------  -----  \n",
            " 0   claim_id        3137 non-null   int64  \n",
            " 1   user_id         3137 non-null   int64  \n",
            " 2   policy_type     3137 non-null   object \n",
            " 3   claim_date      3137 non-null   object \n",
            " 4   claim_amount    3137 non-null   float64\n",
            " 5   was_fraudulent  3137 non-null   bool   \n",
            "dtypes: bool(1), float64(1), int64(2), object(2)\n",
            "memory usage: 125.7+ KB\n",
            "None\n",
            "Missing data:\n",
            " claim_id        0\n",
            "user_id         0\n",
            "policy_type     0\n",
            "claim_date      0\n",
            "claim_amount    0\n",
            "dtype: int64\n",
            "               count unique         top  freq         mean          std  \\\n",
            "claim_id      3137.0    NaN         NaN   NaN       1568.0   905.718223   \n",
            "user_id       3137.0    NaN         NaN   NaN  4954.425247  2872.539376   \n",
            "policy_type     3137      3         Pet  2397          NaN          NaN   \n",
            "claim_date      3137    719  2023-08-12    11          NaN          NaN   \n",
            "claim_amount  3137.0    NaN         NaN   NaN  1157.605865  1754.746022   \n",
            "\n",
            "                min     25%     50%      75%       max  \n",
            "claim_id        0.0   784.0  1568.0   2352.0    3136.0  \n",
            "user_id         1.0  2429.0  4986.0   7392.0    9998.0  \n",
            "policy_type     NaN     NaN     NaN      NaN       NaN  \n",
            "claim_date      NaN     NaN     NaN      NaN       NaN  \n",
            "claim_amount  30.96   290.3  557.56  1185.36  23154.59  \n",
            "\n",
            "telematics shape: (1769092, 9)\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1769092 entries, 0 to 1769091\n",
            "Data columns (total 9 columns):\n",
            " #   Column                     Dtype  \n",
            "---  ------                     -----  \n",
            " 0   user_id                    int64  \n",
            " 1   trip_id                    int64  \n",
            " 2   timestamp_start            object \n",
            " 3   timestamp_end              object \n",
            " 4   distance_miles             float64\n",
            " 5   hard_braking_events        int64  \n",
            " 6   speeding_events            int64  \n",
            " 7   time_of_day                object \n",
            " 8   phone_usage_while_driving  bool   \n",
            "dtypes: bool(1), float64(1), int64(4), object(3)\n",
            "memory usage: 109.7+ MB\n",
            "None\n",
            "Missing data:\n",
            " user_id            0\n",
            "trip_id            0\n",
            "timestamp_start    0\n",
            "timestamp_end      0\n",
            "distance_miles     0\n",
            "dtype: int64\n",
            "                     count   unique                  top freq         mean  \\\n",
            "user_id          1769092.0      NaN                  NaN  NaN  5022.869434   \n",
            "trip_id          1769092.0      NaN                  NaN  NaN     884545.5   \n",
            "timestamp_start    1769092  1731145  2023-05-31 04:34:01    4          NaN   \n",
            "timestamp_end      1769092  1731093  2023-10-04 05:56:27    4          NaN   \n",
            "distance_miles   1769092.0      NaN                  NaN  NaN    10.000869   \n",
            "\n",
            "                           std   min        25%       50%         75%  \\\n",
            "user_id            2881.810354   2.0     2546.0    5039.0      7499.0   \n",
            "trip_id          510693.015549   0.0  442272.75  884545.5  1326818.25   \n",
            "timestamp_start            NaN   NaN        NaN       NaN         NaN   \n",
            "timestamp_end              NaN   NaN        NaN       NaN         NaN   \n",
            "distance_miles        7.079214  0.01       4.81      8.39       13.46   \n",
            "\n",
            "                       max  \n",
            "user_id             9995.0  \n",
            "trip_id          1769091.0  \n",
            "timestamp_start        NaN  \n",
            "timestamp_end          NaN  \n",
            "distance_miles       90.11  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================\n",
        "# PRE-FLIGHT: dataset sanity, targets, alignment, quick leakage scan\n",
        "# (safe to run multiple times)\n",
        "# ============================\n",
        "import os, numpy as np, pandas as pd\n",
        "\n",
        "BASE_DIR = \"/content/dataset/dataset\"\n",
        "TRAIN_DIR = os.path.join(BASE_DIR, \"train\")\n",
        "TEST_DIR  = os.path.join(BASE_DIR, \"test\")\n",
        "\n",
        "def lread(path):\n",
        "    return pd.read_csv(path, encoding=\"latin-1\")\n",
        "\n",
        "# ---- 1) load minimal tables we need here ----\n",
        "users_tr   = lread(os.path.join(TRAIN_DIR, \"users.csv\"))\n",
        "claims_tr  = lread(os.path.join(TRAIN_DIR, \"claims_history.csv\"))\n",
        "wear_tr    = lread(os.path.join(TRAIN_DIR, \"wearables.csv\"))\n",
        "dig_tr     = lread(os.path.join(TRAIN_DIR, \"digital_behavior.csv\"))\n",
        "tel_tr     = lread(os.path.join(TRAIN_DIR, \"telematics.csv\"))\n",
        "cur_tr     = lread(os.path.join(TRAIN_DIR, \"current_products.csv\"))\n",
        "\n",
        "users_te   = lread(os.path.join(TEST_DIR, \"users.csv\"))\n",
        "wear_te    = lread(os.path.join(TEST_DIR, \"wearables.csv\"))\n",
        "dig_te     = lread(os.path.join(TEST_DIR, \"digital_behavior.csv\"))\n",
        "tel_te     = lread(os.path.join(TEST_DIR, \"telematics.csv\"))\n",
        "cur_te     = lread(os.path.join(TEST_DIR, \"current_products.csv\"))\n",
        "\n",
        "# ---- 2) quick shapes & key checks ----\n",
        "def brief(df, name, key=\"user_id\"):\n",
        "    print(f\"{name:20s} shape={df.shape} | {key} unique={df[key].nunique() if key in df.columns else 'NA'}\")\n",
        "\n",
        "print(\"\\n== TRAIN shapes ==\")\n",
        "for df,name in [(users_tr,\"users\"),(claims_tr,\"claims_history\"),(wear_tr,\"wearables\"),\n",
        "                (dig_tr,\"digital_behavior\"),(tel_tr,\"telematics\"),(cur_tr,\"current_products\")]:\n",
        "    brief(df,name)\n",
        "\n",
        "print(\"\\n== TEST shapes ==\")\n",
        "for df,name in [(users_te,\"users\"),(wear_te,\"wearables\"),\n",
        "                (dig_te,\"digital_behavior\"),(tel_te,\"telematics\"),(cur_te,\"current_products\")]:\n",
        "    brief(df,name)\n",
        "\n",
        "# required columns\n",
        "required_users = {\"user_id\"}\n",
        "assert required_users.issubset(users_tr.columns), \"users.csv missing user_id\"\n",
        "assert \"user_id\" in users_te.columns, \"test users.csv missing user_id\"\n",
        "\n",
        "# ---- 3) target preview (frequency per policy) ----\n",
        "claims_tr[\"policy_type\"] = claims_tr[\"policy_type\"].str.lower()\n",
        "freq = (\n",
        "    claims_tr.groupby([\"user_id\",\"policy_type\"])[\"claim_id\"].count()\n",
        "    .unstack(\"policy_type\").fillna(0)\n",
        ")\n",
        "freq_bin = (freq > 0).astype(int)\n",
        "base_rates = freq_bin.mean().sort_values(ascending=False)\n",
        "print(\"\\n== frequency base rates (train) ==\")\n",
        "print(base_rates.rename(\"positive_rate\"))\n",
        "\n",
        "# ---- 4) build tiny feature master (just to see join cardinality) ----\n",
        "def agg_minimal(df, name):\n",
        "    num = df.select_dtypes(include=\"number\").columns.tolist()\n",
        "    if \"user_id\" in num: num.remove(\"user_id\")\n",
        "    if not num:\n",
        "        return pd.DataFrame({\"user_id\": df[\"user_id\"].unique()})\n",
        "    g = df.groupby(\"user_id\")[num].agg([\"mean\",\"std\"]).reset_index()\n",
        "    g.columns = [\"user_id\"] + [f\"{name}__{a}_{b}\" for a,b in g.columns.tolist()[1:]]\n",
        "    return g\n",
        "\n",
        "feat = users_tr.merge(agg_minimal(wear_tr,\"wear\"), on=\"user_id\", how=\"left\") \\\n",
        "               .merge(agg_minimal(dig_tr,\"dig\"),  on=\"user_id\", how=\"left\") \\\n",
        "               .merge(agg_minimal(tel_tr,\"tel\"),  on=\"user_id\", how=\"left\") \\\n",
        "               .merge(cur_tr.drop(columns=[c for c in [\"portfolio_asset_allocation\"] if c in cur_tr.columns]),\n",
        "                      on=\"user_id\", how=\"left\")\n",
        "\n",
        "print(f\"\\n== feature master preview ==\")\n",
        "print(\"feature_master shape:\", feat.shape)\n",
        "print(\"null % (top 10):\")\n",
        "print((feat.isnull().mean().sort_values(ascending=False).head(10) * 100).round(2))\n",
        "\n",
        "# ---- 5) quick leakage probe (single-policy) ----\n",
        "probe_y = freq_bin.get(\"pet\", pd.Series(0, index=freq_bin.index))  # choose one policy\n",
        "tmp = feat.set_index(\"user_id\").loc[probe_y.index].copy()\n",
        "# kill any column that looks obviously leaky by name\n",
        "bad_like = [c for c in tmp.columns if any(x in c.lower() for x in [\"claim\",\"freq\",\"sev\",\"target\",\"label\",\"policy_type\"])]\n",
        "tmp = tmp.drop(columns=bad_like, errors=\"ignore\")\n",
        "\n",
        "# simple 1D correlation scan on numeric cols\n",
        "num_cols = tmp.select_dtypes(include=\"number\").columns\n",
        "corrs = []\n",
        "for c in num_cols:\n",
        "    try:\n",
        "        r = np.corrcoef(tmp[c].fillna(tmp[c].median()), probe_y)[0,1]\n",
        "        if abs(r) >= 0.98:\n",
        "            corrs.append((c, float(r)))\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "if corrs:\n",
        "    print(\"\\nðŸš¨ high correlation columns (possible leakage):\")\n",
        "    print(corrs[:10])\n",
        "else:\n",
        "    print(\"\\nâœ… no obvious leakage by simple correlation probe.\")\n",
        "\n",
        "# ---- 6) train/test alignment sanity ----\n",
        "feat_te = users_te.merge(agg_minimal(wear_te,\"wear\"), on=\"user_id\", how=\"left\") \\\n",
        "                  .merge(agg_minimal(dig_te,\"dig\"),   on=\"user_id\", how=\"left\") \\\n",
        "                  .merge(agg_minimal(tel_te,\"tel\"),   on=\"user_id\", how=\"left\") \\\n",
        "                  .merge(cur_te.drop(columns=[c for c in [\"portfolio_asset_allocation\"] if c in cur_te.columns]),\n",
        "                         on=\"user_id\", how=\"left\")\n",
        "\n",
        "train_cols = set(feat.columns) - {\"user_id\"}\n",
        "test_cols  = set(feat_te.columns) - {\"user_id\"}\n",
        "missing_in_test = sorted(list(train_cols - test_cols))\n",
        "extra_in_test   = sorted(list(test_cols - train_cols))\n",
        "\n",
        "print(\"\\n== alignment check ==\")\n",
        "print(\"missing_in_test (should be empty or very small):\", missing_in_test[:20])\n",
        "print(\"extra_in_test   (ok):\", extra_in_test[:20])\n",
        "\n",
        "if len(missing_in_test) == 0:\n",
        "    print(\"\\nâœ… alignment looks good. You can start training.\")\n",
        "else:\n",
        "    print(\"\\nâš ï¸ some train features not present in test. The training code will auto reindex, but review if many.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N80zW3q5-3je",
        "outputId": "d5567d94-e2e5-4cc0-ebf7-72f03cdeed2a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "== TRAIN shapes ==\n",
            "users                shape=(10000, 19) | user_id unique=10000\n",
            "claims_history       shape=(3137, 6) | user_id unique=2341\n",
            "wearables            shape=(3655000, 6) | user_id unique=5000\n",
            "digital_behavior     shape=(1050000, 7) | user_id unique=10000\n",
            "telematics           shape=(1769092, 9) | user_id unique=4000\n",
            "current_products     shape=(10000, 8) | user_id unique=10000\n",
            "\n",
            "== TEST shapes ==\n",
            "users                shape=(4000, 19) | user_id unique=4000\n",
            "wearables            shape=(1462000, 6) | user_id unique=2000\n",
            "digital_behavior     shape=(420000, 7) | user_id unique=4000\n",
            "telematics           shape=(719420, 9) | user_id unique=1600\n",
            "current_products     shape=(4000, 8) | user_id unique=4000\n",
            "\n",
            "== frequency base rates (train) ==\n",
            "policy_type\n",
            "pet        0.762067\n",
            "auto       0.213157\n",
            "renters    0.082016\n",
            "Name: positive_rate, dtype: float64\n",
            "\n",
            "== feature master preview ==\n",
            "feature_master shape: (10000, 47)\n",
            "null % (top 10):\n",
            "tel__trip_id_mean                 60.0\n",
            "tel__distance_miles_std           60.0\n",
            "tel__distance_miles_mean          60.0\n",
            "tel__hard_braking_events_mean     60.0\n",
            "tel__hard_braking_events_std      60.0\n",
            "tel__speeding_events_std          60.0\n",
            "tel__speeding_events_mean         60.0\n",
            "tel__trip_id_std                  60.0\n",
            "wear__sleep_duration_hours_std    50.0\n",
            "wear__active_minutes_mean         50.0\n",
            "dtype: float64\n",
            "\n",
            "âœ… no obvious leakage by simple correlation probe.\n",
            "\n",
            "== alignment check ==\n",
            "missing_in_test (should be empty or very small): []\n",
            "extra_in_test   (ok): []\n",
            "\n",
            "âœ… alignment looks good. You can start training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install catboost ngboost\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xpfGMP3j-8FE",
        "outputId": "936f6d0b-5533-4945-af07-c712d7481fe0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m99.2/99.2 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m349.3/349.3 kB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m117.3/117.3 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for autograd-gamma (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# â­ FREQUENCYâ€“SEVERITY SUPER ENSEMBLE (CPU, leak-proof)\n",
        "# - Robust preprocessing (winsor + rare-bucket + OHE, version-proof)\n",
        "# - OOF CV bagging across seeds (LGBM, XGB, CatBoost*, NGBoost*)\n",
        "# - Isotonic calibration, blend + meta-learner (LogReg) for P*\n",
        "# - Stacked regressors for SEV* (LGBM Tweedie + Linear, XGB Pseudo-Huber, Cat*, NGB*)\n",
        "# - Sensible thresholds (Youden + cost-aware)\n",
        "# - Safe fallbacks everywhere (no silent breaks)\n",
        "# * optional: auto-skipped if not installed\n",
        "# ============================================\n",
        "\n",
        "import os, gc, ast, json, warnings, inspect\n",
        "import numpy as np, pandas as pd\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold, KFold\n",
        "from sklearn.metrics import roc_auc_score, roc_curve, mean_squared_error\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.isotonic import IsotonicRegression\n",
        "from sklearn.linear_model import Ridge, LogisticRegression\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "import lightgbm as lgb\n",
        "import xgboost as xgb\n",
        "def _try(name):\n",
        "    try: return __import__(name)\n",
        "    except Exception: return None\n",
        "catboost = _try(\"catboost\")\n",
        "ngboost  = _try(\"ngboost\")\n",
        "\n",
        "import joblib\n",
        "\n",
        "# ---------------------------\n",
        "# CONFIG\n",
        "# ---------------------------\n",
        "BASE_DIR   = \"/content/dataset/dataset\"\n",
        "TRAIN_DIR  = os.path.join(BASE_DIR, \"train\")\n",
        "TEST_DIR   = os.path.join(BASE_DIR, \"test\")\n",
        "\n",
        "N_FOLDS      = 5\n",
        "BAG_SEEDS    = [42, 1337, 2025]\n",
        "RANDOM_STATE = 42\n",
        "np.random.seed(RANDOM_STATE)\n",
        "FAST_MODE    = False   # True = quick dev; False = higher accuracy\n",
        "N_JOBS       = -1\n",
        "\n",
        "COST_FP = 1.0\n",
        "COST_FN = 5.0\n",
        "\n",
        "LEAKY_FRAGS = [\n",
        "    'claim', 'policy_type', 'fraud', 'settle', 'payout', 'accident', 'incident', 'loss',\n",
        "    'freq_', 'sev_', 'sev_log_', 'target', 'label'\n",
        "]\n",
        "\n",
        "# ---------------------------\n",
        "# UTILS\n",
        "# ---------------------------\n",
        "def load_csv(p): return pd.read_csv(p, encoding=\"latin-1\")\n",
        "def rmse(y, yhat):\n",
        "    try: return float(mean_squared_error(y, yhat, squared=False))\n",
        "    except TypeError: return float(mean_squared_error(y, yhat)**0.5)\n",
        "\n",
        "def youden_thr(y, p):\n",
        "    fpr, tpr, thr = roc_curve(y, p)\n",
        "    return float(thr[np.argmax(tpr - fpr)])\n",
        "\n",
        "def best_cost_thr(y, p, steps=400):\n",
        "    y = (np.asarray(y) > 0.5).astype(int)\n",
        "    grid = np.linspace(0,1,steps)\n",
        "    best_t, best_cost = 0.5, 1e18\n",
        "    for t in grid:\n",
        "        pred = (p >= t).astype(int)\n",
        "        fp = ((pred==1)&(y==0)).sum()\n",
        "        fn = ((pred==0)&(y==1)).sum()\n",
        "        cost = COST_FP*fp + COST_FN*fn\n",
        "        if cost < best_cost: best_cost, best_t = cost, float(t)\n",
        "    return best_t, best_cost\n",
        "\n",
        "# ---------------------------\n",
        "# TRANSFORMERS\n",
        "# ---------------------------\n",
        "class Winsorizer(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, q=(0.01,0.99)): self.q=q; self.b={}\n",
        "    def fit(self, X, y=None):\n",
        "        X=pd.DataFrame(X);\n",
        "        for c in X.columns:\n",
        "            s = pd.to_numeric(X[c], errors=\"coerce\")\n",
        "            self.b[c]=(np.nanquantile(s,self.q[0]), np.nanquantile(s,self.q[1]))\n",
        "        return self\n",
        "    def transform(self, X):\n",
        "        X=pd.DataFrame(X);\n",
        "        for c,(lo,hi) in self.b.items():\n",
        "            X[c]=pd.to_numeric(X[c], errors=\"coerce\").clip(lo,hi)\n",
        "        return X.values\n",
        "\n",
        "class RareBucket(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, min_count=20): self.min=min_count; self.keep={}\n",
        "    def fit(self, X, y=None):\n",
        "        X=pd.DataFrame(X).astype(\"object\")\n",
        "        for c in X.columns:\n",
        "            vc=X[c].value_counts(dropna=False)\n",
        "            self.keep[c]=set(vc[vc>=self.min].index)\n",
        "        return self\n",
        "    def transform(self, X):\n",
        "        X=pd.DataFrame(X).astype(\"object\")\n",
        "        for c,keep in self.keep.items():\n",
        "            X[c]=X[c].apply(lambda v: v if v in keep else \"Other\")\n",
        "        return X.values\n",
        "\n",
        "# ---------------------------\n",
        "# FEATURE ENGINEERING (leak-proof)\n",
        "# ---------------------------\n",
        "def _safe_get(df, col, default=0): return df[col] if col in df.columns else default\n",
        "\n",
        "def extract_json(df, col, prefix):\n",
        "    if col not in df.columns: return df\n",
        "    def safe_eval(x):\n",
        "        if pd.isna(x): return {}\n",
        "        if isinstance(x, dict): return x\n",
        "        if isinstance(x,str):\n",
        "            try: return ast.literal_eval(x)\n",
        "            except Exception: return {}\n",
        "        return {}\n",
        "    s=df[col].apply(safe_eval)\n",
        "    def sumv(d):\n",
        "        try: return float(sum(v for v in d.values() if pd.notna(v)))\n",
        "        except: return 0.0\n",
        "    df[f\"{prefix}_vector_sum\"]=s.apply(sumv)\n",
        "    keys=['US Stocks','International Stocks','Bonds'] if prefix=='alloc' else []\n",
        "    for k in keys: df[f\"{prefix}_{k.lower().replace(' ','_')}\"]=s.apply(lambda d:d.get(k,0))\n",
        "    return df.drop(columns=[col], errors=\"ignore\")\n",
        "\n",
        "def aggregate(path, name):\n",
        "    df=load_csv(path)\n",
        "    if 'user_id' not in df.columns: raise ValueError(f\"user_id missing in {path}\")\n",
        "    num=df.select_dtypes(include='number').columns.tolist()\n",
        "    if 'user_id'in num: num.remove('user_id')\n",
        "    if not num: return pd.DataFrame({'user_id':df['user_id'].unique()})\n",
        "    g=df.groupby('user_id')[num].agg(['mean','std','min','max']).reset_index()\n",
        "    g.columns=['user_id']+[f\"{name}_{a}_{b}\" for a,b in g.columns.tolist()[1:]]\n",
        "    if name=='telematics':\n",
        "        df['hard_braking_events']=_safe_get(df,'hard_braking_events',0)\n",
        "        df['speeding_events']=_safe_get(df,'speeding_events',0)\n",
        "        df['total_events']=df['hard_braking_events']+df['speeding_events']\n",
        "        ev=df.groupby('user_id')['total_events'].mean().reset_index()\n",
        "        ev.columns=['user_id','telematics_events_per_trip_mean']\n",
        "        g=g.merge(ev,on='user_id',how='left')\n",
        "    return g\n",
        "\n",
        "def create_feature_master(folder):\n",
        "    print(f\"Loading data from {folder}...\")\n",
        "    users=load_csv(os.path.join(folder,'users.csv')).drop_duplicates('user_id',keep='first')\n",
        "    wear =aggregate(os.path.join(folder,'wearables.csv'),'wearable')\n",
        "    digi =aggregate(os.path.join(folder,'digital_behavior.csv'),'digital')\n",
        "    tele=aggregate(os.path.join(folder,'telematics.csv'),'telematics')\n",
        "    cur  =load_csv(os.path.join(folder,'current_products.csv'))\n",
        "    cur  =extract_json(cur,'portfolio_asset_allocation','alloc')\n",
        "\n",
        "    feat=users.merge(wear,on='user_id',how='left')\\\n",
        "              .merge(digi,on='user_id',how='left')\\\n",
        "              .merge(tele,on='user_id',how='left')\\\n",
        "              .merge(cur,on='user_id',how='left')\n",
        "\n",
        "    for c in ['annual_income','student_loan_debt','credit_card_debt',\n",
        "              'risk_tolerance_score','financial_literacy_score']:\n",
        "        if c not in feat.columns: feat[c]=np.nan\n",
        "    feat['financial_health']=feat['annual_income'].fillna(0)-feat['student_loan_debt'].fillna(0)-feat['credit_card_debt'].fillna(0)\n",
        "    feat['risk_propensity']=feat['risk_tolerance_score'].fillna(0)*feat['financial_literacy_score'].fillna(0)\n",
        "\n",
        "    bad=[c for c in feat.columns if any(t in c.lower() for t in LEAKY_FRAGS)]\n",
        "    if bad:\n",
        "        print(f\"âš ï¸ Removing potential leak columns: {bad[:12]}{' ...' if len(bad)>12 else ''}\")\n",
        "        feat=feat.drop(columns=bad, errors='ignore')\n",
        "    return feat\n",
        "\n",
        "def build_targets(feature_master, claims_history):\n",
        "    ch=claims_history.copy()\n",
        "    if 'policy_type'in ch.columns: ch['policy_type']=ch['policy_type'].str.lower()\n",
        "    agg=ch.groupby(['user_id','policy_type']).agg(freq=('claim_id','count'),\n",
        "                                                  sev_base=('claim_amount','mean')).reset_index()\n",
        "    pv=agg.pivot_table(index='user_id', columns='policy_type',\n",
        "                       values=['freq','sev_base']).reset_index()\n",
        "    cols=list(pv.columns)\n",
        "    if isinstance(cols[0],tuple) and cols[0][0]=='user_id': pv.rename(columns={cols[0]:'user_id'}, inplace=True)\n",
        "    new=['user_id']+[f\"{a}_{str(b).lower()}\" if isinstance((a,b),tuple) else a for a,b in pv.columns[1:]]\n",
        "    pv.columns=new\n",
        "    tgt=feature_master[['user_id']].merge(pv,on='user_id',how='left')\n",
        "    for p in ['auto','renters','pet']:\n",
        "        if f'freq_{p}' not in tgt.columns: tgt[f'freq_{p}']=0\n",
        "        tgt[f'freq_{p}']=tgt[f'freq_{p}'].fillna(0).gt(0).astype(float)\n",
        "        col=f'sev_base_{p}'\n",
        "        if col not in tgt.columns: tgt[col]=np.nan\n",
        "        tgt[f'sev_log_{p}']=np.log1p(np.clip(tgt[col].astype(float),0,None))\n",
        "        tgt.drop(columns=[col], inplace=True, errors='ignore')\n",
        "    return tgt\n",
        "\n",
        "def build_Xy(feature_master, targets):\n",
        "    feat_cols=[c for c in feature_master.columns if c!='user_id' and not any(k in c.lower() for k in ['freq_','sev_','sev_log_'])]\n",
        "    X=feature_master[feat_cols].copy()\n",
        "    df=targets.copy()\n",
        "    y_pack={\n",
        "        'auto':    df.get('freq_auto',    pd.Series(0,index=df.index)).astype(float).values,\n",
        "        'renters': df.get('freq_renters', pd.Series(0,index=df.index)).astype(float).values,\n",
        "        'pet':     df.get('freq_pet',     pd.Series(0,index=df.index)).astype(float).values,\n",
        "        'sev_log_auto':    df.get('sev_log_auto',    pd.Series(np.nan,index=df.index)).astype(float).values,\n",
        "        'sev_log_renters': df.get('sev_log_renters', pd.Series(np.nan,index=df.index)).astype(float).values,\n",
        "        'sev_log_pet':     df.get('sev_log_pet',     pd.Series(np.nan,index=df.index)).astype(float).values,\n",
        "    }\n",
        "    return X, y_pack\n",
        "\n",
        "# ---------------------------\n",
        "# PREPROCESSOR (version-proof)\n",
        "# ---------------------------\n",
        "def _make_ohe(min_freq=20):\n",
        "    sig=inspect.signature(OneHotEncoder.__init__)\n",
        "    kw={}\n",
        "    if 'min_frequency'in sig.parameters:\n",
        "        kw['handle_unknown']='infrequent_if_exist'; kw['min_frequency']=min_freq\n",
        "    else:\n",
        "        kw['handle_unknown']='ignore'\n",
        "    if 'sparse_output'in sig.parameters: kw['sparse_output']=True\n",
        "    else: kw['sparse']=True\n",
        "    return OneHotEncoder(**kw)\n",
        "\n",
        "def get_preprocessor(X):\n",
        "    X=X.copy()\n",
        "    for c in X.select_dtypes(include='bool').columns: X[c]=X[c].astype(int)\n",
        "    num_cols=X.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    cat_cols=X.select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "    num=Pipeline([('imputer',SimpleImputer(strategy='median')),\n",
        "                  ('winsor',Winsorizer((0.01,0.99))),\n",
        "                  ('scaler',StandardScaler())])\n",
        "\n",
        "    sig=inspect.signature(OneHotEncoder.__init__)\n",
        "    cat_steps=[('imputer',SimpleImputer(strategy='most_frequent'))]\n",
        "    if 'min_frequency' not in sig.parameters:\n",
        "        cat_steps.append(('rare',RareBucket(min_count=20)))\n",
        "    cat_steps.append(('onehot',_make_ohe(20)))\n",
        "    cat=Pipeline(cat_steps)\n",
        "\n",
        "    return ColumnTransformer([('num',num,num_cols),('cat',cat,cat_cols)],\n",
        "                             remainder='drop', sparse_threshold=0.3)\n",
        "\n",
        "# ---------------------------\n",
        "# MODELS\n",
        "# ---------------------------\n",
        "def _n_est(): return 1200 if FAST_MODE else 3000\n",
        "def _lr():    return 0.03  if FAST_MODE else 0.015\n",
        "\n",
        "def class_weights(y):\n",
        "    y=np.asarray(y).astype(float)\n",
        "    pos=y.sum(); neg=len(y)-pos\n",
        "    spw=neg/max(pos,1.0); cw=None if (pos==0 or neg==0) else {0:1.0,1:neg/max(pos,1.0)}\n",
        "    return spw, cw\n",
        "\n",
        "# classifiers\n",
        "def lgbm_clf(cw):\n",
        "    return lgb.LGBMClassifier(n_estimators=_n_est(), learning_rate=_lr(),\n",
        "        max_depth=7, num_leaves=48, min_child_samples=80,\n",
        "        subsample=0.8, colsample_bytree=0.7, reg_alpha=0.3, reg_lambda=1.5,\n",
        "        objective='binary', metric='auc', class_weight=cw, n_jobs=N_JOBS,\n",
        "        random_state=RANDOM_STATE, verbose=-1)\n",
        "\n",
        "def xgb_clf(spw):\n",
        "    return xgb.XGBClassifier(n_estimators=_n_est(), learning_rate=_lr(),\n",
        "        max_depth=6, min_child_weight=6, subsample=0.8, colsample_bytree=0.7,\n",
        "        reg_lambda=2.0, gamma=0.15, objective='binary:logistic', eval_metric='auc',\n",
        "        tree_method='hist', predictor='auto', n_jobs=N_JOBS, random_state=RANDOM_STATE,\n",
        "        scale_pos_weight=spw)\n",
        "\n",
        "def cat_clf():\n",
        "    if catboost is None: return None\n",
        "    return catboost.CatBoostClassifier(iterations=_n_est(), learning_rate=_lr(),\n",
        "        depth=7, l2_leaf_reg=5.0, loss_function='Logloss', eval_metric='AUC',\n",
        "        random_seed=RANDOM_STATE, thread_count=max(1,os.cpu_count()-1), verbose=False)\n",
        "\n",
        "def ngb_clf():\n",
        "    if ngboost is None: return None\n",
        "    from ngboost.distns import Bernoulli\n",
        "    from ngboost.scores import LogScore\n",
        "    return ngboost.NGBClassifier(Dist=Bernoulli, Score=LogScore,\n",
        "        n_estimators=max(400,_n_est()//6), learning_rate=min(0.05,_lr()*2.0),\n",
        "        natural_gradient=True, verbose=False, random_state=RANDOM_STATE)\n",
        "\n",
        "# regressors\n",
        "def lgbm_reg_tweedie():\n",
        "    return lgb.LGBMRegressor(n_estimators=_n_est(), learning_rate=_lr(),\n",
        "        max_depth=8, num_leaves=96, min_child_samples=50,\n",
        "        subsample=0.8, colsample_bytree=0.7, reg_alpha=0.3, reg_lambda=1.5,\n",
        "        objective='tweedie', tweedie_variance_power=1.3, metric='rmse',\n",
        "        n_jobs=N_JOBS, random_state=RANDOM_STATE, verbose=-1)\n",
        "\n",
        "def lgbm_reg_linear():\n",
        "    return lgb.LGBMRegressor(n_estimators=_n_est(), learning_rate=_lr(),\n",
        "        max_depth=8, num_leaves=96, min_child_samples=50,\n",
        "        subsample=0.8, colsample_bytree=0.7, reg_alpha=0.3, reg_lambda=1.5,\n",
        "        objective='regression', metric='rmse', n_jobs=N_JOBS,\n",
        "        random_state=RANDOM_STATE, verbose=-1)\n",
        "\n",
        "def xgb_reg_phuber():\n",
        "    return xgb.XGBRegressor(n_estimators=_n_est(), learning_rate=_lr(),\n",
        "        max_depth=7, min_child_weight=6, subsample=0.8, colsample_bytree=0.7,\n",
        "        reg_lambda=2.5, gamma=0.1, objective='reg:pseudohubererror', eval_metric='rmse',\n",
        "        tree_method='hist', predictor='auto', n_jobs=N_JOBS, random_state=RANDOM_STATE)\n",
        "\n",
        "def cat_reg():\n",
        "    if catboost is None: return None\n",
        "    return catboost.CatBoostRegressor(iterations=_n_est(), learning_rate=_lr(),\n",
        "        depth=8, l2_leaf_reg=6.0, loss_function='RMSE', random_seed=RANDOM_STATE,\n",
        "        thread_count=max(1,os.cpu_count()-1), verbose=False)\n",
        "\n",
        "def ngb_reg():\n",
        "    if ngboost is None: return None\n",
        "    from ngboost.distns import Normal\n",
        "    from ngboost.scores import LogScore\n",
        "    return ngboost.NGBRegressor(Dist=Normal, Score=LogScore,\n",
        "        n_estimators=max(400,_n_est()//6), learning_rate=min(0.05,_lr()*2.0),\n",
        "        natural_gradient=True, verbose=False, random_state=RANDOM_STATE)\n",
        "\n",
        "# early stopping helpers\n",
        "def fit_lgb(model, Xtr, ytr, Xva, yva):\n",
        "    if isinstance(model, lgb.LGBMModel):\n",
        "        model.fit(Xtr,ytr, eval_set=[(Xva,yva)],\n",
        "                  callbacks=[lgb.log_evaluation(0),\n",
        "                             lgb.early_stopping(stopping_rounds=200, verbose=False)])\n",
        "        return model\n",
        "    model.fit(Xtr,ytr); return model\n",
        "\n",
        "def fit_xgb(model, Xtr, ytr, Xva, yva, maximize=True, metric='auc'):\n",
        "    sig=inspect.signature(model.fit)\n",
        "    if 'early_stopping_rounds' in sig.parameters:\n",
        "        model.fit(Xtr,ytr, eval_set=[(Xva,yva)], verbose=False, early_stopping_rounds=200)\n",
        "    else:\n",
        "        try:\n",
        "            from xgboost.callback import EarlyStopping\n",
        "            cb=EarlyStopping(rounds=200, save_best=True, maximize=maximize,\n",
        "                             data_name='validation_0', metric_name=metric)\n",
        "            model.fit(Xtr,ytr, eval_set=[(Xva,yva)], verbose=False, callbacks=[cb])\n",
        "        except Exception:\n",
        "            model.fit(Xtr,ytr, eval_set=[(Xva,yva)], verbose=False)\n",
        "    return model\n",
        "\n",
        "# ---------------------------\n",
        "# LEAKAGE SCAN (quick)\n",
        "# ---------------------------\n",
        "def leakage_scan_drop(X, y_bin):\n",
        "    bad=[]\n",
        "    y=np.asarray(y_bin).astype(int)\n",
        "    for col in X.columns:\n",
        "        try:\n",
        "            xx=X[[col]]\n",
        "            num=xx.select_dtypes(include=[np.number]).columns.tolist()\n",
        "            cat=xx.select_dtypes(include=['object']).columns.tolist()\n",
        "            steps=[]\n",
        "            if num: steps.append(('num',Pipeline([('imp',SimpleImputer(strategy='median')),('sc',StandardScaler())]),num))\n",
        "            if cat: steps.append(('cat',Pipeline([('imp',SimpleImputer(strategy='most_frequent')),('ohe',_make_ohe(5))]),cat))\n",
        "            if not steps: continue\n",
        "            pp=ColumnTransformer(steps, remainder='drop')\n",
        "            xp=pp.fit_transform(xx)\n",
        "            clf=LogisticRegression(max_iter=200)\n",
        "            clf.fit(xp,y)\n",
        "            p=clf.predict_proba(xp)[:,1]\n",
        "            if roc_auc_score(y,p)>=0.98: bad.append(col)\n",
        "        except Exception:\n",
        "            pass\n",
        "    if bad:\n",
        "        print(f\"ðŸš¨ Potential leakage-like columns (AUCâ‰¥0.98): {bad}\")\n",
        "        X=X.drop(columns=bad, errors='ignore')\n",
        "    return X, bad\n",
        "\n",
        "# ---------------------------\n",
        "# OOF SUPERBAG â€” FREQUENCY\n",
        "# ---------------------------\n",
        "def oof_freq_superbag(prep, X, y, tag):\n",
        "    Xp=prep.transform(X)\n",
        "    y=np.asarray(y).astype(int)           # <-- FIX for NGBoost & scoring\n",
        "    spw,cw=class_weights(y)\n",
        "\n",
        "    builders=[('lgbm', lambda:lgbm_clf(cw)),\n",
        "              ('xgb',  lambda:xgb_clf(spw))]\n",
        "    if catboost is not None: builders.append(('cat', cat_clf))\n",
        "    if ngboost  is not None: builders.append(('ngb', ngb_clf))\n",
        "\n",
        "    oof_by_model={n:np.zeros(len(y)) for n,_ in [(n,b) for n,b in builders]}\n",
        "    models_by_seed=[]; calibs_by_seed=[]; seed_oofs=[]; seed_thr_y=[]; seed_thr_c=[]\n",
        "\n",
        "    for seed in BAG_SEEDS:\n",
        "        skf=StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=seed)\n",
        "        fold_models=[]; fold_calibs=[]; oof_seed=np.zeros(len(y))\n",
        "        for fold,(tr,va) in enumerate(skf.split(Xp,y),1):\n",
        "            Xtr,Xva=Xp[tr],Xp[va]; ytr,yva=y[tr],y[va]\n",
        "            per=[]; pack={}; cals={}\n",
        "            for name,mk in [(n,b) for n,b in builders]:\n",
        "                m=mk()\n",
        "                if name=='xgb': m=fit_xgb(m,Xtr,ytr,Xva,yva,True,'auc')\n",
        "                elif name=='lgbm': m=fit_lgb(m,Xtr,ytr,Xva,yva)\n",
        "                else: m.fit(Xtr,ytr)\n",
        "                if hasattr(m,'predict_proba'): p=m.predict_proba(Xva)[:,1]\n",
        "                else:\n",
        "                    try: p=m.predict_proba(Xva)[:,1]\n",
        "                    except: p=1/(1+np.exp(-m.predict(Xva)))\n",
        "                iso=IsotonicRegression(out_of_bounds='clip'); iso.fit(p,yva)\n",
        "                pcal=iso.transform(p)\n",
        "                auc=roc_auc_score(yva,pcal)\n",
        "                print(f\"[{tag}:{name}] seed={seed} fold={fold} AUC={auc:.4f}\")\n",
        "                if auc>=0.99: raise RuntimeError(f\"ðŸ›‘ ABORT: [{tag}:{name}] leak AUC={auc:.4f}\")\n",
        "                oof_by_model[name][va]=pcal; per.append(pcal)\n",
        "                pack[name]=m; cals[name]=iso\n",
        "            blend=np.mean(np.vstack(per),axis=0)\n",
        "            oof_seed[va]=blend\n",
        "            fold_models.append(pack); fold_calibs.append(cals)\n",
        "        models_by_seed.append(fold_models); calibs_by_seed.append(fold_calibs)\n",
        "        auc=roc_auc_score(y,oof_seed); ty=youden_thr(y,oof_seed); tc,_=best_cost_thr(y,oof_seed)\n",
        "        print(f\"[{tag}] seed={seed} OOF AUC={auc:.4f} | thr_y={ty:.4f} | thr_c={tc:.4f}\")\n",
        "        seed_oofs.append(oof_seed); seed_thr_y.append(ty); seed_thr_c.append(tc)\n",
        "\n",
        "    base_names=[n for n,_ in [(n,b) for n,b in builders]]\n",
        "    X_meta=np.vstack([oof_by_model[n] for n in base_names]).T\n",
        "    meta=LogisticRegression(max_iter=1000); meta.fit(X_meta,y)\n",
        "    oof_meta=meta.predict_proba(X_meta)[:,1]; auc_meta=roc_auc_score(y,oof_meta)\n",
        "\n",
        "    oof_blend=np.mean(np.vstack(seed_oofs),axis=0); auc_blend=roc_auc_score(y,oof_blend)\n",
        "    print(f\"[{tag}] META AUC={auc_meta:.4f} | BAG BLEND AUC={auc_blend:.4f}\")\n",
        "\n",
        "    pd.DataFrame({**{f'oof_{n}':oof_by_model[n] for n in base_names},\n",
        "                  'oof_blend':oof_blend,'oof_meta':oof_meta,'y':y}).to_csv(f\"oof_{tag.replace('*_','')}_freq.csv\",index=False)\n",
        "\n",
        "    return {'builders':base_names,'models_by_seed':models_by_seed,'calibs_by_seed':calibs_by_seed,\n",
        "            'meta':meta,'auc_oof_meta':float(auc_meta),'auc_oof_blend':float(auc_blend),\n",
        "            'thr_youden':float(np.mean(seed_thr_y)),'thr_cost':float(np.mean(seed_thr_c))}\n",
        "\n",
        "def predict_freq(bundle, prep, X):\n",
        "    Xp=prep.transform(X)\n",
        "    names=bundle['builders']; seed_preds=[]\n",
        "    for si,fold_models in enumerate(bundle['models_by_seed']):\n",
        "        fold_pred=np.zeros(Xp.shape[0]); nfold=0\n",
        "        for fold_pack, fold_cals in zip(fold_models, bundle['calibs_by_seed'][si]):\n",
        "            per=[]\n",
        "            for n in names:\n",
        "                m=fold_pack[n]\n",
        "                if hasattr(m,'predict_proba'): p=m.predict_proba(Xp)[:,1]\n",
        "                else:\n",
        "                    try: p=m.predict_proba(Xp)[:,1]\n",
        "                    except: p=1/(1+np.exp(-m.predict(Xp)))\n",
        "                per.append(fold_cals[n].transform(p))\n",
        "            fold_pred += np.mean(np.vstack(per),axis=0); nfold+=1\n",
        "        seed_preds.append(fold_pred/max(1,nfold))\n",
        "    blend=np.mean(np.vstack(seed_preds),axis=0)\n",
        "\n",
        "    # meta features from first seed+fold for stability\n",
        "    try:\n",
        "        pack0=bundle['models_by_seed'][0][0]; cal0=bundle['calibs_by_seed'][0][0]\n",
        "        feats=[]\n",
        "        for n in names:\n",
        "            m=pack0[n]\n",
        "            if hasattr(m,'predict_proba'): p=m.predict_proba(Xp)[:,1]\n",
        "            else:\n",
        "                try: p=m.predict_proba(Xp)[:,1]\n",
        "                except: p=1/(1+np.exp(-m.predict(Xp)))\n",
        "            feats.append(cal0[n].transform(p))\n",
        "        Xmeta=np.vstack(feats).T\n",
        "        pmeta=bundle['meta'].predict_proba(Xmeta)[:,1]\n",
        "        prob=0.5*blend+0.5*pmeta\n",
        "    except Exception:\n",
        "        prob=blend\n",
        "    return np.clip(prob,0.0,1.0)\n",
        "\n",
        "# ---------------------------\n",
        "# OOF SUPERBAG â€” SEVERITY\n",
        "# ---------------------------\n",
        "def oof_sev_superbag(prep, X, ylog, tag):\n",
        "    Xp=prep.transform(X)\n",
        "    builders=[('lgb_tweedie', lgbm_reg_tweedie),\n",
        "              ('xgb_phuber',  xgb_reg_phuber),\n",
        "              ('lgb_linear',  lgbm_reg_linear)]\n",
        "    if catboost is not None: builders.append(('cat', cat_reg))\n",
        "    if ngboost  is not None: builders.append(('ngb', ngb_reg))\n",
        "\n",
        "    seed_packs=[]; all_oofs=[]\n",
        "    for seed in BAG_SEEDS:\n",
        "        kf=KFold(n_splits=N_FOLDS, shuffle=True, random_state=seed)\n",
        "        oof_stack=np.zeros((len(ylog), len(builders)))\n",
        "        fold_models=[]\n",
        "        for fold,(tr,va) in enumerate(kf.split(Xp),1):\n",
        "            Xtr,Xva=Xp[tr],Xp[va]; ytr,yva=ylog[tr],ylog[va]\n",
        "            models=[]\n",
        "            for j,(name,mk) in enumerate(builders):\n",
        "                m=mk()\n",
        "                if isinstance(m, xgb.XGBRegressor): m=fit_xgb(m,Xtr,ytr,Xva,yva,False,'rmse')\n",
        "                elif isinstance(m, lgb.LGBMRegressor): m=fit_lgb(m,Xtr,ytr,Xva,yva)\n",
        "                else: m.fit(Xtr,ytr)\n",
        "                oof_stack[va,j]=m.predict(Xva)\n",
        "                models.append(m)\n",
        "            fold_models.append(models)\n",
        "        meta=Ridge(alpha=1.0, random_state=RANDOM_STATE); meta.fit(oof_stack,ylog)\n",
        "        oof_meta=meta.predict(oof_stack); print(f\"[{tag}] seed={seed} OOF RMSE(log)={rmse(ylog,oof_meta):.4f}\")\n",
        "        seed_packs.append({'models_by_fold':fold_models,'meta':meta}); all_oofs.append(oof_meta)\n",
        "\n",
        "    bag_rmse=float(np.mean([rmse(ylog,o) for o in all_oofs])); print(f\"[{tag}] BAG OOF RMSE(log)={bag_rmse:.4f}\")\n",
        "    pd.DataFrame({'oof_meta_bag':np.mean(np.vstack(all_oofs),axis=0),'y_log':ylog}).to_csv(f\"oof_{tag.replace('*_','')}_sev.csv\",index=False)\n",
        "    return {'builders':[n for n,_ in builders],'seeds':seed_packs,'rmse_oof_log':bag_rmse}\n",
        "\n",
        "def predict_sev(bundle, prep, X):\n",
        "    Xp=prep.transform(X); names=bundle['builders']; out=np.zeros(Xp.shape[0])\n",
        "    for seed_pack in bundle['seeds']:\n",
        "        fold_models=seed_pack['models_by_fold']; meta=seed_pack['meta']\n",
        "        base=np.zeros((Xp.shape[0], len(names)))\n",
        "        for models in fold_models:\n",
        "            for j,m in enumerate(models): base[:,j]+=m.predict(Xp)\n",
        "        base/=max(1,len(fold_models)); out+=meta.predict(base)\n",
        "    out/=max(1,len(bundle['seeds']))\n",
        "    sev=np.expm1(out); return np.maximum(1.0, sev)\n",
        "\n",
        "def stabilize(sev, shrink=0.15, cap_q=0.995):\n",
        "    med=np.nanmedian(sev); s=(1-shrink)*sev+shrink*med; hi=np.nanquantile(s,cap_q)\n",
        "    return np.clip(s,1.0,hi)\n",
        "\n",
        "# ---------------------------\n",
        "# MAIN\n",
        "# ---------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"=\"*60); print(\"=== PHASE 1: LEAK-PROOF DATA PREP (FREQUENCY-SEVERITY) ===\"); print(\"=\"*60)\n",
        "    claims = load_csv(os.path.join(TRAIN_DIR,'claims_history.csv'))\n",
        "    feat_tr = create_feature_master(TRAIN_DIR)\n",
        "    targets = build_targets(feat_tr, claims)\n",
        "    X_tr, y = build_Xy(feat_tr, targets)\n",
        "\n",
        "    # quick leakage probe using 'pet' as target\n",
        "    X_tr, dropped = leakage_scan_drop(X_tr, y['pet'])\n",
        "    if dropped: print(f\"âœ… Dropped suspicious columns: {dropped}\")\n",
        "\n",
        "    pre = get_preprocessor(X_tr); pre.fit(X_tr)\n",
        "    del claims, targets; gc.collect()\n",
        "\n",
        "    print(\"\\n\"+\"=\"*60); print(\"=== PHASE 2: SUPER ENSEMBLE OOF TRAINING (CPU) ===\"); print(\"=\"*60)\n",
        "    bundles={}; metrics={}\n",
        "    for disp,key in [('Auto','auto'),('Renters','renters'),('Pet','pet')]:\n",
        "        print(f\"\\n--- {disp} ---\")\n",
        "        y_freq = y[key].astype(float); print(f\"  -> P* positives: {int(y_freq.sum())}/{len(y_freq)}\")\n",
        "        freq_b = oof_freq_superbag(pre, X_tr, y_freq, f\"P*_{disp}\")\n",
        "        bundles[f'p_{key}']=freq_b\n",
        "        metrics[f'auc_oof_p_{key}_blend']=freq_b['auc_oof_blend']\n",
        "        metrics[f'auc_oof_p_{key}_meta']=freq_b['auc_oof_meta']\n",
        "        metrics[f'thr_youden_p_{key}']=freq_b['thr_youden']\n",
        "        metrics[f'thr_cost_p_{key}']=freq_b['thr_cost']\n",
        "\n",
        "        mask=(y_freq==1.0); print(f\"  -> SEV* training size: {int(mask.sum())}\")\n",
        "        if mask.sum()>N_FOLDS:\n",
        "            sev_b=oof_sev_superbag(pre, X_tr.loc[mask], y[f'sev_log_{key}'][mask], f\"SEV*_{disp}\")\n",
        "        else:\n",
        "            class Dummy:\n",
        "                def predict(self, X): return np.zeros(X.shape[0])\n",
        "            sev_b={'builders':[], 'seeds':[{'models_by_fold':[], 'meta':Dummy()}], 'rmse_oof_log':0.0}\n",
        "        bundles[f'sev_{key}']=sev_b\n",
        "        metrics[f'rmse_oof_log_sev_{key}']=sev_b['rmse_oof_log']\n",
        "\n",
        "    joblib.dump(bundles,'super_ensemble_models.joblib')\n",
        "    joblib.dump(pre,'super_preprocessor.joblib')\n",
        "    with open('super_metrics.json','w') as f: json.dump(metrics,f,indent=2)\n",
        "    print(\"\\nâœ… Saved super ensemble models, preprocessor, and OOF metrics.\")\n",
        "    print(\"OOF metrics:\", json.dumps(metrics, indent=2))\n",
        "\n",
        "    print(\"\\n\"+\"=\"*60); print(\"=== PHASE 3: PREDICT & SUBMIT ===\"); print(\"=\"*60)\n",
        "    feat_te=create_feature_master(TEST_DIR)\n",
        "    if 'user_id' not in feat_te.columns: raise ValueError(\"user_id missing in test data\")\n",
        "    X_te = feat_te.drop(columns=['user_id'], errors='ignore').reindex(columns=X_tr.columns, fill_value=np.nan)\n",
        "\n",
        "    sub=pd.DataFrame({'user_id':feat_te['user_id']})\n",
        "    for key in ['auto','renters','pet']:\n",
        "        p = predict_freq(bundles[f'p_{key}'], pre, X_te)\n",
        "        sub[f'p_{key}']=np.clip(p,0.001,0.999)\n",
        "        sraw = predict_sev(bundles[f'sev_{key}'], pre, X_te)\n",
        "        sub[f'sev_{key}']=stabilize(sraw, shrink=0.15, cap_q=0.995)\n",
        "\n",
        "    cols=['user_id','p_auto','p_renters','p_pet','sev_auto','sev_renters','sev_pet']\n",
        "    sub[cols].to_csv('submission_predictions_super.csv', index=False)\n",
        "    print(\"\\nâœ… Created 'submission_predictions_super.csv'. Head:\\n\", sub[cols].head())\n",
        "    print(\"\\nðŸŽ¯ Done. Leak-proof, calibrated, bagged, stacked â€” with safe fallbacks.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "VMOwh3Ev_RTj",
        "outputId": "6348b639-76c5-45f4-caf2-fc038b274c75"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "=== PHASE 1: LEAK-PROOF DATA PREP (FREQUENCY-SEVERITY) ===\n",
            "============================================================\n",
            "Loading data from /content/dataset/dataset/train...\n",
            "\n",
            "============================================================\n",
            "=== PHASE 2: SUPER ENSEMBLE OOF TRAINING (CPU) ===\n",
            "============================================================\n",
            "\n",
            "--- Auto ---\n",
            "  -> P* positives: 499/10000\n",
            "[P*_Auto:lgbm] seed=42 fold=1 AUC=0.8673\n",
            "[P*_Auto:xgb] seed=42 fold=1 AUC=0.8460\n",
            "[P*_Auto:cat] seed=42 fold=1 AUC=0.8508\n",
            "[P*_Auto:ngb] seed=42 fold=1 AUC=0.8599\n",
            "[P*_Auto:lgbm] seed=42 fold=2 AUC=0.8770\n",
            "[P*_Auto:xgb] seed=42 fold=2 AUC=0.8329\n",
            "[P*_Auto:cat] seed=42 fold=2 AUC=0.8420\n",
            "[P*_Auto:ngb] seed=42 fold=2 AUC=0.8597\n",
            "[P*_Auto:lgbm] seed=42 fold=3 AUC=0.8565\n",
            "[P*_Auto:xgb] seed=42 fold=3 AUC=0.8548\n",
            "[P*_Auto:cat] seed=42 fold=3 AUC=0.8614\n",
            "[P*_Auto:ngb] seed=42 fold=3 AUC=0.8603\n",
            "[P*_Auto:lgbm] seed=42 fold=4 AUC=0.8530\n",
            "[P*_Auto:xgb] seed=42 fold=4 AUC=0.8351\n",
            "[P*_Auto:cat] seed=42 fold=4 AUC=0.8399\n",
            "[P*_Auto:ngb] seed=42 fold=4 AUC=0.8428\n",
            "[P*_Auto:lgbm] seed=42 fold=5 AUC=0.8992\n",
            "[P*_Auto:xgb] seed=42 fold=5 AUC=0.8766\n",
            "[P*_Auto:cat] seed=42 fold=5 AUC=0.8685\n",
            "[P*_Auto:ngb] seed=42 fold=5 AUC=0.8832\n",
            "[P*_Auto] seed=42 OOF AUC=0.8764 | thr_y=0.0814 | thr_c=0.1353\n",
            "[P*_Auto:lgbm] seed=1337 fold=1 AUC=0.8844\n",
            "[P*_Auto:xgb] seed=1337 fold=1 AUC=0.8622\n",
            "[P*_Auto:cat] seed=1337 fold=1 AUC=0.8689\n",
            "[P*_Auto:ngb] seed=1337 fold=1 AUC=0.8759\n",
            "[P*_Auto:lgbm] seed=1337 fold=2 AUC=0.8499\n",
            "[P*_Auto:xgb] seed=1337 fold=2 AUC=0.8297\n",
            "[P*_Auto:cat] seed=1337 fold=2 AUC=0.8260\n",
            "[P*_Auto:ngb] seed=1337 fold=2 AUC=0.8388\n",
            "[P*_Auto:lgbm] seed=1337 fold=3 AUC=0.8683\n",
            "[P*_Auto:xgb] seed=1337 fold=3 AUC=0.8464\n",
            "[P*_Auto:cat] seed=1337 fold=3 AUC=0.8545\n",
            "[P*_Auto:ngb] seed=1337 fold=3 AUC=0.8650\n",
            "[P*_Auto:lgbm] seed=1337 fold=4 AUC=0.8843\n",
            "[P*_Auto:xgb] seed=1337 fold=4 AUC=0.8384\n",
            "[P*_Auto:cat] seed=1337 fold=4 AUC=0.8605\n",
            "[P*_Auto:ngb] seed=1337 fold=4 AUC=0.8571\n",
            "[P*_Auto:lgbm] seed=1337 fold=5 AUC=0.8642\n",
            "[P*_Auto:xgb] seed=1337 fold=5 AUC=0.8380\n",
            "[P*_Auto:cat] seed=1337 fold=5 AUC=0.8481\n",
            "[P*_Auto:ngb] seed=1337 fold=5 AUC=0.8675\n",
            "[P*_Auto] seed=1337 OOF AUC=0.8763 | thr_y=0.0792 | thr_c=0.1679\n",
            "[P*_Auto:lgbm] seed=2025 fold=1 AUC=0.8666\n",
            "[P*_Auto:xgb] seed=2025 fold=1 AUC=0.8363\n",
            "[P*_Auto:cat] seed=2025 fold=1 AUC=0.8465\n",
            "[P*_Auto:ngb] seed=2025 fold=1 AUC=0.8516\n",
            "[P*_Auto:lgbm] seed=2025 fold=2 AUC=0.8778\n",
            "[P*_Auto:xgb] seed=2025 fold=2 AUC=0.8601\n",
            "[P*_Auto:cat] seed=2025 fold=2 AUC=0.8693\n",
            "[P*_Auto:ngb] seed=2025 fold=2 AUC=0.8751\n",
            "[P*_Auto:lgbm] seed=2025 fold=3 AUC=0.8786\n",
            "[P*_Auto:xgb] seed=2025 fold=3 AUC=0.8422\n",
            "[P*_Auto:cat] seed=2025 fold=3 AUC=0.8463\n",
            "[P*_Auto:ngb] seed=2025 fold=3 AUC=0.8593\n",
            "[P*_Auto:lgbm] seed=2025 fold=4 AUC=0.8781\n",
            "[P*_Auto:xgb] seed=2025 fold=4 AUC=0.8600\n",
            "[P*_Auto:cat] seed=2025 fold=4 AUC=0.8527\n",
            "[P*_Auto:ngb] seed=2025 fold=4 AUC=0.8635\n",
            "[P*_Auto:lgbm] seed=2025 fold=5 AUC=0.8725\n",
            "[P*_Auto:xgb] seed=2025 fold=5 AUC=0.8298\n",
            "[P*_Auto:cat] seed=2025 fold=5 AUC=0.8334\n",
            "[P*_Auto:ngb] seed=2025 fold=5 AUC=0.8571\n",
            "[P*_Auto] seed=2025 OOF AUC=0.8758 | thr_y=0.0711 | thr_c=0.1529\n",
            "[P*_Auto] META AUC=0.8767 | BAG BLEND AUC=0.8812\n",
            "  -> SEV* training size: 499\n",
            "[SEV*_Auto] seed=42 OOF RMSE(log)=0.6954\n",
            "[SEV*_Auto] seed=1337 OOF RMSE(log)=0.6931\n",
            "[SEV*_Auto] seed=2025 OOF RMSE(log)=0.6961\n",
            "[SEV*_Auto] BAG OOF RMSE(log)=0.6949\n",
            "\n",
            "--- Renters ---\n",
            "  -> P* positives: 192/10000\n",
            "[P*_Renters:lgbm] seed=42 fold=1 AUC=0.8796\n",
            "[P*_Renters:xgb] seed=42 fold=1 AUC=0.8584\n",
            "[P*_Renters:cat] seed=42 fold=1 AUC=0.8785\n",
            "[P*_Renters:ngb] seed=42 fold=1 AUC=0.8773\n",
            "[P*_Renters:lgbm] seed=42 fold=2 AUC=0.8761\n",
            "[P*_Renters:xgb] seed=42 fold=2 AUC=0.8672\n",
            "[P*_Renters:cat] seed=42 fold=2 AUC=0.8692\n",
            "[P*_Renters:ngb] seed=42 fold=2 AUC=0.8637\n",
            "[P*_Renters:lgbm] seed=42 fold=3 AUC=0.8775\n",
            "[P*_Renters:xgb] seed=42 fold=3 AUC=0.8667\n",
            "[P*_Renters:cat] seed=42 fold=3 AUC=0.8668\n",
            "[P*_Renters:ngb] seed=42 fold=3 AUC=0.8789\n",
            "[P*_Renters:lgbm] seed=42 fold=4 AUC=0.8666\n",
            "[P*_Renters:xgb] seed=42 fold=4 AUC=0.8745\n",
            "[P*_Renters:cat] seed=42 fold=4 AUC=0.8731\n",
            "[P*_Renters:ngb] seed=42 fold=4 AUC=0.8638\n",
            "[P*_Renters:lgbm] seed=42 fold=5 AUC=0.8778\n",
            "[P*_Renters:xgb] seed=42 fold=5 AUC=0.8824\n",
            "[P*_Renters:cat] seed=42 fold=5 AUC=0.8817\n",
            "[P*_Renters:ngb] seed=42 fold=5 AUC=0.8826\n",
            "[P*_Renters] seed=42 OOF AUC=0.8895 | thr_y=0.0389 | thr_c=0.0977\n",
            "[P*_Renters:lgbm] seed=1337 fold=1 AUC=0.8935\n",
            "[P*_Renters:xgb] seed=1337 fold=1 AUC=0.8931\n",
            "[P*_Renters:cat] seed=1337 fold=1 AUC=0.8962\n",
            "[P*_Renters:ngb] seed=1337 fold=1 AUC=0.9054\n",
            "[P*_Renters:lgbm] seed=1337 fold=2 AUC=0.8797\n",
            "[P*_Renters:xgb] seed=1337 fold=2 AUC=0.8494\n",
            "[P*_Renters:cat] seed=1337 fold=2 AUC=0.8678\n",
            "[P*_Renters:ngb] seed=1337 fold=2 AUC=0.8693\n",
            "[P*_Renters:lgbm] seed=1337 fold=3 AUC=0.8782\n",
            "[P*_Renters:xgb] seed=1337 fold=3 AUC=0.8794\n",
            "[P*_Renters:cat] seed=1337 fold=3 AUC=0.8799\n",
            "[P*_Renters:ngb] seed=1337 fold=3 AUC=0.8574\n",
            "[P*_Renters:lgbm] seed=1337 fold=4 AUC=0.8750\n",
            "[P*_Renters:xgb] seed=1337 fold=4 AUC=0.8696\n",
            "[P*_Renters:cat] seed=1337 fold=4 AUC=0.8661\n",
            "[P*_Renters:ngb] seed=1337 fold=4 AUC=0.8666\n",
            "[P*_Renters:lgbm] seed=1337 fold=5 AUC=0.8498\n",
            "[P*_Renters:xgb] seed=1337 fold=5 AUC=0.8371\n",
            "[P*_Renters:cat] seed=1337 fold=5 AUC=0.8585\n",
            "[P*_Renters:ngb] seed=1337 fold=5 AUC=0.8582\n",
            "[P*_Renters] seed=1337 OOF AUC=0.8854 | thr_y=0.0358 | thr_c=0.1153\n",
            "[P*_Renters:lgbm] seed=2025 fold=1 AUC=0.8751\n",
            "[P*_Renters:xgb] seed=2025 fold=1 AUC=0.8564\n",
            "[P*_Renters:cat] seed=2025 fold=1 AUC=0.8779\n",
            "[P*_Renters:ngb] seed=2025 fold=1 AUC=0.8581\n",
            "[P*_Renters:lgbm] seed=2025 fold=2 AUC=0.8850\n",
            "[P*_Renters:xgb] seed=2025 fold=2 AUC=0.8638\n",
            "[P*_Renters:cat] seed=2025 fold=2 AUC=0.8884\n",
            "[P*_Renters:ngb] seed=2025 fold=2 AUC=0.8675\n",
            "[P*_Renters:lgbm] seed=2025 fold=3 AUC=0.8731\n",
            "[P*_Renters:xgb] seed=2025 fold=3 AUC=0.8515\n",
            "[P*_Renters:cat] seed=2025 fold=3 AUC=0.8592\n",
            "[P*_Renters:ngb] seed=2025 fold=3 AUC=0.8698\n",
            "[P*_Renters:lgbm] seed=2025 fold=4 AUC=0.8913\n",
            "[P*_Renters:xgb] seed=2025 fold=4 AUC=0.8830\n",
            "[P*_Renters:cat] seed=2025 fold=4 AUC=0.8951\n",
            "[P*_Renters:ngb] seed=2025 fold=4 AUC=0.8837\n",
            "[P*_Renters:lgbm] seed=2025 fold=5 AUC=0.8759\n",
            "[P*_Renters:xgb] seed=2025 fold=5 AUC=0.8598\n",
            "[P*_Renters:cat] seed=2025 fold=5 AUC=0.8732\n",
            "[P*_Renters:ngb] seed=2025 fold=5 AUC=0.8687\n",
            "[P*_Renters] seed=2025 OOF AUC=0.8909 | thr_y=0.0393 | thr_c=0.1278\n",
            "[P*_Renters] META AUC=0.8909 | BAG BLEND AUC=0.8967\n",
            "  -> SEV* training size: 192\n",
            "[SEV*_Renters] seed=42 OOF RMSE(log)=0.4918\n",
            "[SEV*_Renters] seed=1337 OOF RMSE(log)=0.4835\n",
            "[SEV*_Renters] seed=2025 OOF RMSE(log)=0.4928\n",
            "[SEV*_Renters] BAG OOF RMSE(log)=0.4894\n",
            "\n",
            "--- Pet ---\n",
            "  -> P* positives: 1784/10000\n",
            "[P*_Pet:lgbm] seed=42 fold=1 AUC=0.8709\n",
            "[P*_Pet:xgb] seed=42 fold=1 AUC=0.8706\n",
            "[P*_Pet:cat] seed=42 fold=1 AUC=0.8677\n",
            "[P*_Pet:ngb] seed=42 fold=1 AUC=0.8703\n",
            "[P*_Pet:lgbm] seed=42 fold=2 AUC=0.8840\n",
            "[P*_Pet:xgb] seed=42 fold=2 AUC=0.8757\n",
            "[P*_Pet:cat] seed=42 fold=2 AUC=0.8759\n",
            "[P*_Pet:ngb] seed=42 fold=2 AUC=0.8811\n",
            "[P*_Pet:lgbm] seed=42 fold=3 AUC=0.8591\n",
            "[P*_Pet:xgb] seed=42 fold=3 AUC=0.8645\n",
            "[P*_Pet:cat] seed=42 fold=3 AUC=0.8636\n",
            "[P*_Pet:ngb] seed=42 fold=3 AUC=0.8634\n",
            "[P*_Pet:lgbm] seed=42 fold=4 AUC=0.8784\n",
            "[P*_Pet:xgb] seed=42 fold=4 AUC=0.8761\n",
            "[P*_Pet:cat] seed=42 fold=4 AUC=0.8770\n",
            "[P*_Pet:ngb] seed=42 fold=4 AUC=0.8739\n",
            "[P*_Pet:lgbm] seed=42 fold=5 AUC=0.8773\n",
            "[P*_Pet:xgb] seed=42 fold=5 AUC=0.8718\n",
            "[P*_Pet:cat] seed=42 fold=5 AUC=0.8715\n",
            "[P*_Pet:ngb] seed=42 fold=5 AUC=0.8717\n",
            "[P*_Pet] seed=42 OOF AUC=0.8793 | thr_y=0.3292 | thr_c=0.3283\n",
            "[P*_Pet:lgbm] seed=1337 fold=1 AUC=0.8872\n",
            "[P*_Pet:xgb] seed=1337 fold=1 AUC=0.8801\n",
            "[P*_Pet:cat] seed=1337 fold=1 AUC=0.8794\n",
            "[P*_Pet:ngb] seed=1337 fold=1 AUC=0.8846\n",
            "[P*_Pet:lgbm] seed=1337 fold=2 AUC=0.8729\n",
            "[P*_Pet:xgb] seed=1337 fold=2 AUC=0.8712\n",
            "[P*_Pet:cat] seed=1337 fold=2 AUC=0.8721\n",
            "[P*_Pet:ngb] seed=1337 fold=2 AUC=0.8730\n",
            "[P*_Pet:lgbm] seed=1337 fold=3 AUC=0.8716\n",
            "[P*_Pet:xgb] seed=1337 fold=3 AUC=0.8645\n",
            "[P*_Pet:cat] seed=1337 fold=3 AUC=0.8648\n",
            "[P*_Pet:ngb] seed=1337 fold=3 AUC=0.8646\n",
            "[P*_Pet:lgbm] seed=1337 fold=4 AUC=0.8723\n",
            "[P*_Pet:xgb] seed=1337 fold=4 AUC=0.8746\n",
            "[P*_Pet:cat] seed=1337 fold=4 AUC=0.8735\n",
            "[P*_Pet:ngb] seed=1337 fold=4 AUC=0.8741\n",
            "[P*_Pet:lgbm] seed=1337 fold=5 AUC=0.8763\n",
            "[P*_Pet:xgb] seed=1337 fold=5 AUC=0.8743\n",
            "[P*_Pet:cat] seed=1337 fold=5 AUC=0.8715\n",
            "[P*_Pet:ngb] seed=1337 fold=5 AUC=0.8683\n",
            "[P*_Pet] seed=1337 OOF AUC=0.8815 | thr_y=0.3341 | thr_c=0.3258\n",
            "[P*_Pet:lgbm] seed=2025 fold=1 AUC=0.8795\n",
            "[P*_Pet:xgb] seed=2025 fold=1 AUC=0.8727\n",
            "[P*_Pet:cat] seed=2025 fold=1 AUC=0.8749\n",
            "[P*_Pet:ngb] seed=2025 fold=1 AUC=0.8722\n",
            "[P*_Pet:lgbm] seed=2025 fold=2 AUC=0.8802\n",
            "[P*_Pet:xgb] seed=2025 fold=2 AUC=0.8844\n",
            "[P*_Pet:cat] seed=2025 fold=2 AUC=0.8842\n",
            "[P*_Pet:ngb] seed=2025 fold=2 AUC=0.8799\n",
            "[P*_Pet:lgbm] seed=2025 fold=3 AUC=0.8762\n",
            "[P*_Pet:xgb] seed=2025 fold=3 AUC=0.8732\n",
            "[P*_Pet:cat] seed=2025 fold=3 AUC=0.8674\n",
            "[P*_Pet:ngb] seed=2025 fold=3 AUC=0.8694\n",
            "[P*_Pet:lgbm] seed=2025 fold=4 AUC=0.8740\n",
            "[P*_Pet:xgb] seed=2025 fold=4 AUC=0.8782\n",
            "[P*_Pet:cat] seed=2025 fold=4 AUC=0.8760\n",
            "[P*_Pet:ngb] seed=2025 fold=4 AUC=0.8738\n",
            "[P*_Pet:lgbm] seed=2025 fold=5 AUC=0.8715\n",
            "[P*_Pet:xgb] seed=2025 fold=5 AUC=0.8645\n",
            "[P*_Pet:cat] seed=2025 fold=5 AUC=0.8674\n",
            "[P*_Pet:ngb] seed=2025 fold=5 AUC=0.8657\n",
            "[P*_Pet] seed=2025 OOF AUC=0.8815 | thr_y=0.3297 | thr_c=0.3258\n",
            "[P*_Pet] META AUC=0.8816 | BAG BLEND AUC=0.8869\n",
            "  -> SEV* training size: 1784\n",
            "[SEV*_Pet] seed=42 OOF RMSE(log)=0.7512\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2106585118.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    551\u001b[0m         \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_freq\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"  -> SEV* training size: {int(mask.sum())}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0mN_FOLDS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 553\u001b[0;31m             \u001b[0msev_b\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moof_sev_superbag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpre\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_tr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mf'sev_log_{key}'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"SEV*_{disp}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    554\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m             \u001b[0;32mclass\u001b[0m \u001b[0mDummy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2106585118.py\u001b[0m in \u001b[0;36moof_sev_superbag\u001b[0;34m(prep, X, ylog, tag)\u001b[0m\n\u001b[1;32m    493\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXGBRegressor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfit_xgb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mXtr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mytr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mXva\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myva\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'rmse'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLGBMRegressor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfit_lgb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mXtr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mytr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mXva\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myva\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 495\u001b[0;31m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXtr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mytr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    496\u001b[0m                 \u001b[0moof_stack\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mva\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXva\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m                 \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/catboost/core.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, cat_features, text_features, embedding_features, graph, sample_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001b[0m\n\u001b[1;32m   5871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'loss_function'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5872\u001b[0m             \u001b[0mCatBoostRegressor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_is_compatible_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss_function'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5873\u001b[0;31m         return self._fit(X, y, cat_features, text_features, embedding_features, None, graph, sample_weight, None, None, None, None, baseline,\n\u001b[0m\u001b[1;32m   5874\u001b[0m                          \u001b[0muse_best_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogging_level\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumn_description\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5875\u001b[0m                          \u001b[0mverbose_eval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric_period\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msilent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/catboost/core.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, cat_features, text_features, embedding_features, pairs, graph, sample_weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001b[0m\n\u001b[1;32m   2408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2409\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mplot_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Training plots'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_get_train_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2410\u001b[0;31m                 self._train(\n\u001b[0m\u001b[1;32m   2411\u001b[0m                     \u001b[0mtrain_pool\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2412\u001b[0m                     \u001b[0mtrain_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"eval_sets\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/catboost/core.py\u001b[0m in \u001b[0;36m_train\u001b[0;34m(self, train_pool, test_pool, params, allow_clear_pool, init_model)\u001b[0m\n\u001b[1;32m   1788\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1789\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_clear_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1790\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_object\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_clear_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_object\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0minit_model\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1791\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_trained_model_attributes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1792\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m_catboost.pyx\u001b[0m in \u001b[0;36m_catboost._CatBoost._train\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m_catboost.pyx\u001b[0m in \u001b[0;36m_catboost._CatBoost._train\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== RESUME WHAT'S MISSING â†’ SAVE â†’ PREDICT ====\n",
        "import os, json, joblib, numpy as np, pandas as pd\n",
        "\n",
        "# 0) Try to load partial artifacts if not in RAM\n",
        "if 'bundles' not in globals():\n",
        "    if os.path.exists('super_ensemble_models_PARTIAL.joblib'):\n",
        "        bundles = joblib.load('super_ensemble_models_PARTIAL.joblib')\n",
        "        print(\"â„¹ï¸ Loaded bundles from PARTIAL checkpoint.\")\n",
        "    elif os.path.exists('super_ensemble_models.joblib'):\n",
        "        bundles = joblib.load('super_ensemble_models.joblib')\n",
        "        print(\"â„¹ï¸ Loaded bundles from full checkpoint.\")\n",
        "    else:\n",
        "        bundles = {}\n",
        "\n",
        "if 'preprocessor' not in globals():\n",
        "    if os.path.exists('super_preprocessor_PARTIAL.joblib'):\n",
        "        preprocessor = joblib.load('super_preprocessor_PARTIAL.joblib')\n",
        "        print(\"â„¹ï¸ Loaded preprocessor from PARTIAL checkpoint.\")\n",
        "    elif os.path.exists('super_preprocessor.joblib'):\n",
        "        preprocessor = joblib.load('super_preprocessor.joblib')\n",
        "        print(\"â„¹ï¸ Loaded preprocessor from full checkpoint.\")\n",
        "\n",
        "if 'metrics' not in globals():\n",
        "    metrics = {}\n",
        "    for p in ['super_metrics_PARTIAL.json','super_metrics.json']:\n",
        "        if os.path.exists(p):\n",
        "            try:\n",
        "                metrics.update(json.load(open(p)))\n",
        "                print(f\"â„¹ï¸ Loaded metrics from {p}\")\n",
        "                break\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "# 1) Ensure train matrices exist (quick rebuild if needed)\n",
        "need_data = any(v not in globals() for v in ['X_train_full','y_pack'])\n",
        "if need_data:\n",
        "    claims_history = pd.read_csv(os.path.join(TRAIN_DIR, 'claims_history.csv'), encoding='latin-1')\n",
        "    feat_master    = create_feature_master(TRAIN_DIR)\n",
        "    targets        = build_targets_from_claims(feat_master, claims_history)\n",
        "    X_train_full, y_pack = build_Xy(feat_master, targets)\n",
        "    if 'preprocessor' not in globals():\n",
        "        preprocessor = get_preprocessor(X_train_full)\n",
        "        preprocessor.fit(X_train_full)\n",
        "    print(\"âœ… Rebuilt train features and targets.\")\n",
        "\n",
        "# 2) Train ONLY what is missing\n",
        "def _has(key): return key in bundles and bundles[key]\n",
        "\n",
        "todo = []\n",
        "for k in ['p_auto','sev_auto','p_renters','sev_renters','p_pet','sev_pet']:\n",
        "    if not _has(k): todo.append(k)\n",
        "print(\"â–¶ï¸ To train:\", todo if todo else \"nothing (all done)\")\n",
        "\n",
        "if todo:\n",
        "    # FREQUENCY\n",
        "    if 'p_auto' in todo:\n",
        "        y = y_pack['auto'].astype(float)\n",
        "        bundles['p_auto'] = oof_frequency_superbag(preprocessor, X_train_full, y, \"P*_Auto\")\n",
        "        metrics['auc_oof_p_auto_blend'] = bundles['p_auto']['auc_oof_blend']\n",
        "        metrics['auc_oof_p_auto_meta']  = bundles['p_auto']['auc_oof_meta']\n",
        "    if 'p_renters' in todo:\n",
        "        y = y_pack['renters'].astype(float)\n",
        "        bundles['p_renters'] = oof_frequency_superbag(preprocessor, X_train_full, y, \"P*_Renters\")\n",
        "        metrics['auc_oof_p_renters_blend'] = bundles['p_renters']['auc_oof_blend']\n",
        "        metrics['auc_oof_p_renters_meta']  = bundles['p_renters']['auc_oof_meta']\n",
        "    if 'p_pet' in todo:\n",
        "        y = y_pack['pet'].astype(float)\n",
        "        bundles['p_pet'] = oof_frequency_superbag(preprocessor, X_train_full, y, \"P*_Pet\")\n",
        "        metrics['auc_oof_p_pet_blend'] = bundles['p_pet']['auc_oof_blend']\n",
        "        metrics['auc_oof_p_pet_meta']  = bundles['p_pet']['auc_oof_meta']\n",
        "\n",
        "    # SEVERITY (train only on claimers)\n",
        "    if 'sev_auto' in todo:\n",
        "        yfreq = y_pack['auto'].astype(float)\n",
        "        mask = (yfreq == 1.0)\n",
        "        ylog = y_pack['sev_log_auto'][mask]\n",
        "        bundles['sev_auto'] = oof_severity_superbag(preprocessor, X_train_full.loc[mask], ylog, \"SEV*_Auto\")\n",
        "        metrics['rmse_oof_log_sev_auto'] = bundles['sev_auto']['rmse_oof_log']\n",
        "    if 'sev_renters' in todo:\n",
        "        yfreq = y_pack['renters'].astype(float)\n",
        "        mask = (yfreq == 1.0)\n",
        "        ylog = y_pack['sev_log_renters'][mask]\n",
        "        bundles['sev_renters'] = oof_severity_superbag(preprocessor, X_train_full.loc[mask], ylog, \"SEV*_Renters\")\n",
        "        metrics['rmse_oof_log_sev_renters'] = bundles['sev_renters']['rmse_oof_log']\n",
        "    if 'sev_pet' in todo:\n",
        "        yfreq = y_pack['pet'].astype(float)\n",
        "        mask = (yfreq == 1.0)\n",
        "        ylog = y_pack['sev_log_pet'][mask]\n",
        "        bundles['sev_pet'] = oof_severity_superbag(preprocessor, X_train_full.loc[mask], ylog, \"SEV*_Pet\")\n",
        "        metrics['rmse_oof_log_sev_pet'] = bundles['sev_pet']['rmse_oof_log']\n",
        "\n",
        "    # Save full artifacts\n",
        "    joblib.dump(bundles, 'super_ensemble_models.joblib')\n",
        "    joblib.dump(preprocessor, 'super_preprocessor.joblib')\n",
        "    json.dump(metrics, open('super_metrics.json','w'), indent=2)\n",
        "    # Mirror to Drive if mounted\n",
        "    try:\n",
        "        os.makedirs('/content/drive/MyDrive/model_backups/FINAL', exist_ok=True)\n",
        "        joblib.dump(bundles, '/content/drive/MyDrive/model_backups/FINAL/super_ensemble_models.joblib')\n",
        "        joblib.dump(preprocessor, '/content/drive/MyDrive/model_backups/FINAL/super_preprocessor.joblib')\n",
        "        json.dump(metrics, open('/content/drive/MyDrive/model_backups/FINAL/super_metrics.json','w'), indent=2)\n",
        "        print(\"ðŸ’¾ Saved locally and to Drive/FINAL.\")\n",
        "    except Exception as e:\n",
        "        print(\"Saved locally. Drive mirror skipped:\", e)\n",
        "else:\n",
        "    print(\"âœ… Nothing missing; using existing artifacts.\")\n",
        "\n",
        "# 3) Predict & submit (Phase 3)\n",
        "feat_test = create_feature_master(TEST_DIR)\n",
        "X_test = feat_test.drop(columns=['user_id'], errors='ignore').reindex(\n",
        "    columns=X_train_full.columns, fill_value=np.nan\n",
        ")\n",
        "\n",
        "sub = pd.DataFrame({'user_id': feat_test['user_id']})\n",
        "for key in ['auto','renters','pet']:\n",
        "    prob = predict_frequency_superbag(bundles[f'p_{key}'], preprocessor, X_test)\n",
        "    sub[f'p_{key}'] = np.clip(prob, 0.001, 0.999)\n",
        "\n",
        "    sev_raw = predict_severity_superbag(bundles[f'sev_{key}'], preprocessor, X_test)\n",
        "    sub[f'sev_{key}'] = stabilize_severity(sev_raw, shrink=0.15, cap_quantile=0.995)\n",
        "\n",
        "cols = ['user_id','p_auto','p_renters','p_pet','sev_auto','sev_renters','sev_pet']\n",
        "sub[cols].to_csv('submission_predictions_super.csv', index=False)\n",
        "print(\"âœ… Created 'submission_predictions_super.csv'. Preview:\")\n",
        "print(sub[cols].head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "znPiNBX91Ssi",
        "outputId": "09942fb4-9852-4a62-8844-9c88978ab4dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "â–¶ï¸ To train: ['sev_pet']\n"
          ]
        }
      ]
    }
  ]
}